### Reflective Activity 1 – Ethics in computing in the age of Generative AI

Generative AI has rapidly transformed industries since late 2022, with Computer Science at the heart of this revolution. While AI itself is not new, the fast pace of advancements like ChatGPT has raised urgent concerns about governance. Correa et al. (2023) highlight the challenge of establishing a global consensus on AI regulation, as different countries approach the technology with varying priorities. For example, China focuses on state control, while the European Union prioritizes privacy and human rights through regulations like the AI Act.

The unique nature of generative AI, which can autonomously produce content, creates specific challenges around privacy, intellectual property, and misinformation. Correa et al. argue that policymakers need better tools to compare AI governance strategies worldwide. Without common guidelines, countries may create regulations that either limit innovation or fail to prevent harmful AI misuse.

Deckard (2023) proposes a layered governance model that balances global principles with local flexibility. This would allow international organizations like the UN to set broad ethical guidelines, while individual countries adapt them to fit their legal and cultural contexts. Regulatory sandboxes could help by allowing companies to test AI under controlled conditions.

The governance of generative AI impacts legal, social, and professional fields. Legally, the question of ownership over AI-generated content remains unresolved, especially in cases where it resembles human-made creations. Socially, the rise of deepfakes and manipulated media threatens public trust. Professionally, generative AI could disrupt industries by automating tasks, requiring workers to develop new skills such as AI literacy.
To address these challenges, global collaboration is essential. Establishing an international AI framework could provide common ethical standards while allowing for national adaptation. Public awareness campaigns should educate citizens on AI’s potential risks and benefits, fostering a more informed populace. Additionally, comprehensive legal frameworks must be developed to address intellectual property and data ownership concerns.
By adopting a flexible but structured approach, we can ensure that generative AI evolves in a way that benefits society, minimizes risks, and respects legal and ethical boundaries.

Correa, et al. (2023). [Details to be filled based on actual reference]  
Deckard, (2023). [Details to be filled based on actual reference]


### Collab Discussion 1:

The case study highlights a malware disruption incident, examining the application of the ACM Code of Ethics, which emphasizes the responsibility of computing professionals to prioritize the public good, avoid harm, and uphold trust in the profession. In this case, the ACM Code’s principles would guide the professionals involved to mitigate the harm caused by the malware and ensure that their response is transparent, accountable, and legally compliant.

The first relevant aspect of the ACM Code is the obligation to avoid harm (Principle 2.1). The computing professionals must act swiftly to neutralize the malware while ensuring that their actions do not cause additional damage, such as unauthorized access to systems or breach of privacy. In addition, Principle 2.5 stresses professional responsibility, urging the experts to collaborate with authorities and follow legal guidelines. The malware disruption involves navigating jurisdictional issues since malware often crosses national boundaries. Professionals must be aware of international laws and work within the confines of legal frameworks.

Comparing this to the British Computer Society (BCS) Code of Conduct, the BCS similarly stresses the duty to the public interest and integrity (Section 1), ensuring professionals act lawfully and ethically. Both codes emphasize cooperation with law enforcement and regulatory bodies, highlighting the importance of adhering to local and international laws. In both frameworks, professionalism demands accountability, accuracy, and transparency, ensuring the responsible use of skills to resolve the situation without infringing on rights or escalating harm.
In conclusion, the ethical and legal responsibilities outlined in both the ACM and BCS codes guide computing professionals to act with integrity, safeguard public trust, and comply with legal standards during incidents like malware disruptions.

### Reflective Actvity 2

The Cambridge Analytica scandal in 2018 is a notable example of the inappropriate use of surveys to harvest personal data from millions of Facebook users without informed consent. Cambridge Analytica accessed data through seemingly harmless personality quizzes on Facebook. These quizzes collected not only respondents’ data but also that of their entire social network. The information was used for targeted political campaigns during events such as the 2016 U.S. Presidential Election and the Brexit referendum (Confessore, 2018). The ethical violation here was the manipulation of data collection under false pretenses, misleading users about the scope and purpose of data use.

Ethically, this case is a clear violation of user autonomy and informed consent. Users were unaware that their data was being used for political purposes, breaching principles of honesty and transparency as outlined in the ACM Code of Ethics and the British Computer Society’s (BCS) Code of Conduct. Both emphasize the importance of privacy and obtaining consent for data collection and use.

Legally, the scandal revealed gaps in data protection laws, leading to Facebook facing a $5 billion fine from the U.S. Federal Trade Commission (FTC) and triggering the enforcement of the European Union’s General Data Protection Regulation (GDPR) in 2018. GDPR now mandates stricter rules on data consent and handling, aiming to protect user privacy more effectively.

Socially, the scandal eroded public trust in social media platforms, raising concerns about the influence of big data on democratic processes. The case highlighted how users were profiled and targeted with tailored political ads without their knowledge, sparking global debates on the ethical use of data analytics.

Another example of inappropriate survey use is in health applications, where sensitive medical data is collected through "wellness" surveys and sold to third parties. Similarly, skewed political polling can manipulate public opinion by framing questions to elicit biased responses. 
These examples emphasize the ethical importance of transparency, consent, and responsible data usage. Legal frameworks like GDPR are essential to enforce accountability, while professional bodies require adherence to ethical standards to protect user privacy. Stronger oversight and governance are needed to ensure the ethical use of data collected through surveys, particularly in digital platforms.

